{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lyleregenwetter\\Anaconda3\\envs\\tftest\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/Lyleregenwetter/2.s997-Demo-Materials/main/Classification/BIKED_processed.csv'\n",
    "params = pd.read_csv(url, index_col=0)\n",
    "url='https://raw.githubusercontent.com/Lyleregenwetter/2.s997-Demo-Materials/main/Classification/Bikestyle.csv'\n",
    "classes = pd.read_csv(url, index_col=0)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "params_scaled = min_max_scaler.fit_transform(params.values)\n",
    "params_scaled = pd.DataFrame(params_scaled, columns=params.columns,index=params.index.values).astype('float32')\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(classes)\n",
    "classes_num=le.transform(classes)\n",
    "\n",
    "param_train, param_val, class_train, class_val = train_test_split(params_scaled, classes_num, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dnn(x_dims, y_dims):\n",
    "#     model=tf.keras.Sequential()\n",
    "#     model.add(tf.keras.layers.InputLayer(input_shape=x_dims))\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(200))\n",
    "#     model.add(tf.keras.layers.ReLU())\n",
    "    \n",
    "#     model.add(tf.keras.layers.Dense(200))\n",
    "#     model.add(tf.keras.layers.ReLU())\n",
    "\n",
    "#     model.add(tf.keras.layers.Dense(y_dims))\n",
    "#     model.add(tf.keras.layers.Softmax())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 200)               480400    \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 19)                3819      \n",
      "_________________________________________________________________\n",
      "softmax_3 (Softmax)          (None, 19)                0         \n",
      "=================================================================\n",
      "Total params: 524,419\n",
      "Trainable params: 524,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "37/37 [==============================] - 1s 18ms/step - loss: 2.7061 - accuracy: 0.3904 - val_loss: 2.6061 - val_accuracy: 0.4252\n",
      "Epoch 2/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6233 - accuracy: 0.4079 - val_loss: 2.6059 - val_accuracy: 0.4252\n",
      "Epoch 3/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 4/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 5/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 6/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 7/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 8/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 9/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 10/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 11/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 12/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 13/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 14/1000\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 15/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 16/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 17/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 18/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 19/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 20/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 21/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6232 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 22/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 23/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 24/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 25/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 26/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 27/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 28/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 29/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 30/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 31/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 32/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 33/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 34/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 35/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 36/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 37/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 38/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 39/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 40/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 41/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 42/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 43/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 44/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 45/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 46/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 47/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 48/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 49/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 51/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 52/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 53/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 54/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 55/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 56/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 57/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 58/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 59/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 60/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 61/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 62/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 63/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 64/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 65/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 66/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 67/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 68/1000\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 69/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 70/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 71/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 72/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 73/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 74/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 75/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 76/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 77/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 78/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 79/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 80/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 81/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 82/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 83/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 84/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 85/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 86/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 87/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 88/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 89/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 90/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 91/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 92/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 93/1000\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 94/1000\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 95/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 96/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 97/1000\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 98/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 99/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 100/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 101/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 102/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 103/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 104/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 105/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 106/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 107/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 108/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 109/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 110/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 111/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 112/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 113/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 114/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 115/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 116/1000\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 117/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 118/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 119/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 120/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 121/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 122/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 123/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 124/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 125/1000\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 126/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 127/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 128/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 129/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 130/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 131/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 132/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 133/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 134/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 135/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 136/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 137/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 138/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 139/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 140/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 141/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n",
      "Epoch 142/1000\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 2.6231 - accuracy: 0.4079 - val_loss: 2.6058 - val_accuracy: 0.4252\n"
     ]
    }
   ],
   "source": [
    "# dnnmodel=create_dnn(2401, 19)\n",
    "# dnnmodel.summary()\n",
    "# dnnmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),)\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "# history = dnnmodel.fit(x=param_train, y=class_train, epochs=100, batch_size=100, validation_data=(param_val, class_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [2.7061007022857666, 2.623309850692749, 2.623234987258911, 2.623213768005371, 2.623198986053467, 2.6231861114501953, 2.6231770515441895, 2.6231703758239746, 2.6231653690338135, 2.623161554336548, 2.623159170150757, 2.623157024383545, 2.6231555938720703, 2.623154401779175, 2.6231532096862793, 2.623152494430542, 2.6231513023376465, 2.6231510639190674, 2.6231508255004883, 2.62315034866333, 2.62315034866333, 2.623149871826172, 2.6231496334075928, 2.6231489181518555, 2.6231489181518555, 2.6231486797332764, 2.62314772605896, 2.62314772605896, 2.623147487640381, 2.623147487640381, 2.623147487640381, 2.623147487640381, 2.623147487640381, 2.623147487640381, 2.6231472492218018, 2.623147487640381, 2.6231472492218018, 2.6231472492218018, 2.6231470108032227, 2.6231470108032227, 2.6231470108032227, 2.6231467723846436, 2.6231470108032227, 2.6231470108032227, 2.6231467723846436, 2.6231470108032227, 2.6231467723846436, 2.6231470108032227, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231470108032227, 2.6231467723846436, 2.6231470108032227, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231470108032227, 2.6231470108032227, 2.6231470108032227, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231470108032227, 2.6231470108032227, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436, 2.6231467723846436], 'accuracy': [0.3904128670692444, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826, 0.40786921977996826], 'val_loss': [2.6061277389526367, 2.6058731079101562, 2.6058459281921387, 2.605827808380127, 2.605813503265381, 2.605802297592163, 2.6057939529418945, 2.605787992477417, 2.605783700942993, 2.605780839920044, 2.605778217315674, 2.605776309967041, 2.6057751178741455, 2.60577392578125, 2.6057732105255127, 2.6057727336883545, 2.605771541595459, 2.60577130317688, 2.6057705879211426, 2.6057705879211426, 2.6057701110839844, 2.605769634246826, 2.605769395828247, 2.605769157409668, 2.605768918991089, 2.605768918991089, 2.605768918991089, 2.6057686805725098, 2.6057686805725098, 2.6057686805725098, 2.6057686805725098, 2.6057686805725098, 2.6057686805725098, 2.6057686805725098, 2.6057682037353516, 2.6057679653167725, 2.6057682037353516, 2.6057677268981934, 2.6057679653167725, 2.6057677268981934, 2.6057677268981934, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143, 2.6057674884796143], 'val_accuracy': [0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901, 0.4252491593360901]}\n"
     ]
    }
   ],
   "source": [
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
